{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-C8omlh6Pq4"
   },
   "source": [
    "# **Lab 2: Naive Bayes Classification and Linear Regression**\n",
    "\n",
    "CS 412, Introduction to Machine Learning\n",
    "\n",
    "Department of Computer Science, University of Illinois at Chicago\n",
    "\n",
    "***This is a group work for at most four students.***\n",
    "\n",
    "This is your second lab work, and you will work on it with your teammates. You will learn how to apply the Naive Bayes model to filter spam SMS messages. You will also learn how to make real-valued predictions using a linear regression model. \n",
    "\n",
    "***Deadline:***\n",
    "This assignment is due **Mar 11** (Anywhere on Earth, [AoE](https://www.timeanddate.com/time/zones/aoe)). That is, you can resubmit as often as you like provided that anywhere on Earth is still on or before this date. \n",
    "\n",
    "***How to submit:***\n",
    "See bottom of the page\n",
    "\n",
    "***Python version:***\n",
    "The code should work on Python 3.7 or later, though it might work on earlier versions (not tested). There should be no version problem if you work on Colab.  See a more detailed introduction to Python and Colab at this [link](https://colab.research.google.com/github/cs231n/cs231n.github.io/blob/master/python-colab.ipynb#scrollTo=nxvEkGXPM3Xh).  \n",
    "\n",
    "**Please note before starting the lab:**\n",
    "\n",
    "1. If you use Colab, copy this file to your own Google Drive so that you can edit it.  Ignore the following message when you open the notebook (if it shows up): \\\\\n",
    "`Unrecognized runtime \"python_defaultSpec_1600651579462\"; defaulting to \"python3\"`\n",
    "\n",
    "2. Since the experiments involve randomness, it is important to ensure that your results are replicable. To this end, your implementation should take one integer (or any numeric value) as a seed that is used to initialize the random number generators.\n",
    "See, e.g. [random.seed](https://docs.python.org/3/library/random.html).\n",
    "This has been done for you in the first code block below.\n",
    "\n",
    "3. <font color='red'> There are unit test cases provided after most functions you need to implement. Make good use of them.</font>  For numerical results (i.e., not discrete values), it is fine if your result is within 1% relative difference from the reference result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MuqxFfyO6Pq6"
   },
   "source": [
    "## Problem 1. Naive Bayes Classification **(62 points)** {-}\n",
    "\n",
    "In this problem, you will implement the Naive Bayes classification method and use it for SMS message classifcation. The SMS dataset `SMSSpamCollection` has been provided in the assignment folder, which also can be downloaded from the [UCI Link](http://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection).  In case the repository gets offline occasionally, we have made local copies for the [dataset](https://www.cs.uic.edu/~zhangx/teaching/SMSSpamCollection.dat) and [readme](https://www.cs.uic.edu/~zhangx/teaching/readme.txt).\n",
    "\n",
    "To help you to better understand this algorithm, you are **not** allowed to use any off-the-shelf naive Bayes implementations from third-party libraries. We will implement it with detailed step-by-step instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "io7R6hOn6Pq9"
   },
   "source": [
    "### Recap of the Naive Bayes Algorithm  {-}\n",
    "Naive Bayes classification is a fast and simple classification method. Its efficiency stems from some simplifications we make about the underlying probability distributions, namely, the assumption about the conditional independence of features. Suppose for any class $Y$, we have a probability distribution over all possible combinations of values for a feature vector $X$:\n",
    "$$\n",
    "P(X|Y).\n",
    "$$\n",
    "The main idea of Bayesian classification is to reverse the direction of dependence --- we want to predict the label based on the features:\n",
    "$$\n",
    "P(Y|X).\n",
    "$$\n",
    "This is made possible by the Bayes theorem:\n",
    "\\begin{equation}\n",
    "P(Y|X) = \\frac{P(X|Y)P(Y)}{P(X)}. \\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "To make it more concrete, let us consider the SMS message classification problem. Ignoring punctuations, each SMS message contains an ordered sequence of $T$ words (case-insensitive) $X = \\{X_1, ...,X_T\\}$. That is, $X$ corresponds to an SMS message, and $X_i$ corresponds to the $i$-th word in it. For each message from the training set, there is a corresponding label $Y\\in\\{spam, ham\\}$. \n",
    "\n",
    "**Model specification and key assumption.** The conditional distribution can be written as:\n",
    "$$\n",
    "P(X|Y) = P(X_1, ..., X_T|Y).\n",
    "$$\n",
    "Since this conditional probability is intractable, we simplify it in two steps:\n",
    "\n",
    "1. **Assume** that all features $X_i$ are independent, conditional on the category $Y$. This leads to a naive Bayes model which writes formally as\n",
    "\\begin{equation}\n",
    "P(X|Y) = P(X_1, ..., X_T|Y) = \\prod_{i=1}^T P(X_i|Y). \\tag{2}\n",
    "\\end{equation}\n",
    "\n",
    "2. **Assume** that $P(X_i | Y) = P(X_j|Y)$ for all $i \\neq j$.\n",
    "In other words, given the label $Y$, the value of the $7$-th word has the same distribution as the value of the $10$-th word. Note this is not assumed by default in naive Bayes, and we make this additional assumption to significantly simplify our model.\n",
    "It is often referred to as \"tying\" the probability $P(X_i|Y)$ over $i$.\n",
    "As a result, the order of words no longer matters for $P(X|Y)$,\n",
    "i.e., \n",
    "$$P(X=\\text{'cat is cute'}|Y) \\ \\ = \\ \\ P(X=\\text{'cute cat is'}|Y) \\quad \\text{for all } Y.\n",
    "$$\n",
    "\n",
    "Plugging Eq (2) into the Bayes theorem in Eq (1), we arrive at\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(Y|X) &= \\frac{P(Y) P(X|Y)}{P(X)} = \\frac{P(Y)\\prod_{i=1}^T P(X_i|Y)}{P(X)} \\\\\n",
    "&\\propto P(Y)\\prod_{i=1}^T P(X_i|Y),\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $\\propto$ denotes proportionality. Since the denominator $P(X)$ does not depend on $Y$, the prediction probability is proportional to the numerator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWSd8rJUqhZ7"
   },
   "source": [
    "**Making predictions.**\n",
    "Naturally, given an SMS message $X$, we can first compute $P(Y|X)$ for all possible categories $Y$ (in this example, only two categories), and then make predictions by outputting the $Y$ that maximizes the probability. This can be expressed mathmatically as:\n",
    "$$\n",
    "\\arg\\max_Y P(Y)\\prod_{i=1}^T P(X_i|Y) = \\arg\\max_Y \\left\\{\\log P(Y) + \\sum_{i=1}^T \\log P(X_i|Y) \\right\\}. \\tag{3}\n",
    "$$\n",
    "If there is a tie, we just break it arbitrarily.\n",
    "Here the logarithm uses natural basis.\n",
    "\n",
    "**Learning the model.**\n",
    "To apply the prediction rule in Eq (3), we need to first figure out (formally termed \"estimate\") the value of $P(Y)$ and $P(X_i|Y)$ by using the training data. Recall that since we tie the conditional probabilities $P(X_i|Y)$ across all $i$,\n",
    "the subscript $i$ can be dropped.\n",
    "However, we still carry it just for clarity. \n",
    "\n",
    "Firstly, $P(Y=y)$ can be estimated by computing the frequency of category $y$ in the whole training set ($y$ can be either \"$spam$\" or \"$ham$\"). Here, in order to avoid confusion, we have used the convention that capital letters denote random variables, and lowercase letters denote their possible instantiations.\n",
    "\n",
    "Secondly, $P(X_i = w |Y=y)$ for a word $w$ (e.g., \"cat\") can be estimated by counting the frequency that it appears in the training message set for a given category $y$ (derivation not required in the course):\n",
    "\\begin{align}\n",
    "P(X_i &= w|Y=y) \n",
    "= \\frac{Count(w, y)}{Count(y)}, \\ \\ where\n",
    "\\tag{4} \\\\\n",
    "\\notag\n",
    " Count(w, y) &= \\text{total number of occurrence of $w$ in all SMS messages of category } y \\\\\n",
    "\\notag\n",
    "Count(y) &= \\text{total number of words appearing in SMS messages of category } y.\n",
    "\\end{align}\n",
    "\n",
    "*Remark 1:* If $w$ appears in a single message for 3 times, then it contributes to $Count(w, y)$ by 3, not 1. Similarly, $Count(y)$ indeed equals the total length of all messages in category $y$.\n",
    "\n",
    "*Remark 2:* Obviously, the right-hand side of Eq (4) does not depend on $i$. This is consistent with our previous note that we carry the subscript $i$ in $P(X_i|Y)$ only for clarity, while in fact different $i$ share the same $P(X_i|Y)$.\n",
    "\n",
    "For example, suppose there are four messages \n",
    "$$\n",
    "\\text{{'cat is cute', ham}, {'dog rocks', spam}, {'whatever is is right', ham}, {'hello', spam}.}\n",
    "$$\n",
    "Then $P(X_i = '\\text{is}' | \\text{ham}) = 3 / 7$ (**not** $2/7$),\n",
    "and $P(X_i = '\\text{is}' | \\text{spam}) = 0 / 3$.\n",
    "\n",
    "Quiz (no need to submit): compute $P(X_i = w | \\text{ham})$, for $w = $ 'cat', 'is', 'cute', 'whatever', and 'right'. Check if their sum is 1. Now appreciate why in Remark 1, a word appearing for mulitple times in a single message should be counted multiple times.\n",
    "\n",
    "You may have noticed that any word $w$ with $Count(w,y)=0$ leads to $P(X_i = w|Y=y) = 0$.\n",
    "As a result, by Eq (2), any message $x$ has conditional probably $P(X = x |Y=y) = 0$ if $w$ appears in $x$.\n",
    "Such a \"veto\" is not favorable, and can create significant problems when a word in the test data has never appeared in the training data (think why?).\n",
    "To bypass this issue, we can add pseudo-count, a.k.a additive smoothing:\n",
    "$$\n",
    "\\hat{P}(X_i = w|Y=y) = \\frac{Count(w, y) + \\alpha}{Count(y) + N\\alpha}, \\tag{5}\n",
    "$$\n",
    "where $\\alpha$ is a smoothing parameter. $\\alpha=0$ corresponds to no smoothing. In our experiment, let us set $\\alpha = 1.0$. $N$ denotes the number of distinct words in the vocabulary, and let us set $N = 20,000$ in this lab.\n",
    "\n",
    "Now Let's start with data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 255,
     "status": "ok",
     "timestamp": 1614704206489,
     "user": {
      "displayName": "Xinhua Zhang",
      "photoUrl": "",
      "userId": "16254002428062815681"
     },
     "user_tz": 360
    },
    "id": "TqrY7MaJ6PrA"
   },
   "outputs": [],
   "source": [
    "# set up code for this experiment\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8VYKq9-6PrP"
   },
   "source": [
    "### Data Preprocessing (not for grading) {-}\n",
    "\n",
    "We will use `pandas` to import the dataset. Since `SMSSpamCollection` separates labels and text content in each message by a tab, we will use '\\t' as the value for the `sep` argument and read raw data into a pandas dataframe. As a result, we store labels and SMS messages into two columns. To facilitate the subsequent steps, we also rename the columns by passing a list `['label', 'sms_message']` to the `names` argument of the `read_table()` method.\n",
    "\n",
    "Let us print the first five rows of the dataframe to get a basic understanding of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 782,
     "status": "ok",
     "timestamp": 1614704209062,
     "user": {
      "displayName": "Xinhua Zhang",
      "photoUrl": "",
      "userId": "16254002428062815681"
     },
     "user_tz": 360
    },
    "id": "FXBuWAZ06PrQ",
    "outputId": "9a52c94c-0dcf-4df6-ef4f-eb3f864b8b14"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sms_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                        sms_message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the dataset to the server\n",
    "# Import the data using the read_csv() method from pandas\n",
    "\n",
    "import urllib.request\n",
    "import shutil\n",
    "\n",
    "url = 'https://www.cs.uic.edu/~zhangx/teaching/SMSSpamCollection.dat'\n",
    "file_name = 'SMSSpamCollection.dat'\n",
    "with urllib.request.urlopen(url) as response, open(file_name, 'wb') as out_file:\n",
    "    shutil.copyfileobj(response, out_file)\n",
    "\n",
    "df = pd.read_csv(file_name,\n",
    "                    sep='\\t',\n",
    "                    header=None,\n",
    "                    names=['label', 'sms_message'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzzbwM0h6Prc"
   },
   "source": [
    "#### Step 1: Convert string labels to numerical labels (not for grading) {-}\n",
    "\n",
    "As we can see, there are 2 columns. The first column, which is named `label`, takes two values `spam` (the message is spam) and `ham` (the message is not spam). The second column is the text content of the SMS message that is being classified.  It is a string in which words are separated by space.\n",
    "\n",
    "Note that the string-typed labels are unwieldy for calculating performance metrices, e.g., when calculating precision and recall scores. Hence, let's convert the lables to binary variables, 0 for `ham` and 1 for `spam`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 248,
     "status": "ok",
     "timestamp": 1614704510061,
     "user": {
      "displayName": "Xinhua Zhang",
      "photoUrl": "",
      "userId": "16254002428062815681"
     },
     "user_tz": 360
    },
    "id": "U5myRuvh6Prd",
    "outputId": "fa37b11c-8771-4e60-99b0-3a8dc424286b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sms_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                        sms_message\n",
       "0      0  Go until jurong point, crazy.. Available only ...\n",
       "1      0                      Ok lar... Joking wif u oni...\n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      0  U dun say so early hor... U c already then say...\n",
       "4      0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the next line only once after running the previous code block\n",
    "# Running it more than once will turn the labels into NaN\n",
    "df['label'] = df.label.map({'ham':0, 'spam':1})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2Qsp7_C6Prg"
   },
   "source": [
    "#### Step 2: Bag of words **(10 points)** {-}\n",
    "\n",
    "What we have in our dataset is a large collection of text data (5,572 rows/messages). Most ML algorithms rely on numerical data to be fed into them as input, but SMS messages are usually text heavy. \n",
    "\n",
    "To address this issue, we would like to introduce the concept of Bag of Words (BoW), which is designed for problems with a 'bag of words' or a collection of text data. The basic idea is to count the frequency of the words in the text. It is important to note that BoW treats each word individually, ignoring the order in which the words occur. \n",
    "\n",
    "To count the frequency of the words in text, usually we need to process the input text data in four steps:\n",
    "\n",
    "- Convert all strings into their lower case form\n",
    "- Removing all punctuations\n",
    "- Tokenization, i.e., split a sentence into individual words\n",
    "- Count frequencies\n",
    "\n",
    "Once this has been done, we are supposed to obtain a vocabulary dictionary with frequencies of each words for the given text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Gf3M2dHK6Pri",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hello': 3, 'how': 1, 'are': 1, 'you': 2, 'win': 2, 'money': 1, 'from': 1, 'home': 1, 'call': 2, 'me': 1, 'now': 1, 'tomorrow': 1}\n"
     ]
    }
   ],
   "source": [
    "def count_frequency(documents):\n",
    "    \"\"\"\n",
    "    count occurrence of each word in the document set.\n",
    "    Inputs:\n",
    "    - documents: list, each entity is a string type SMS message\n",
    "    Outputs:\n",
    "    - frequency: a dictionary. The key is the unique words, and the value is the number of occurrences of the word\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Here is the pseudo-code\n",
    "    # Step 1: covert all strings into their lower case form\n",
    "    lower_case_doc = []\n",
    "    for s in documents:\n",
    "        lower_case_doc.append(???)\n",
    "    \n",
    "    # Step 2: remove all punctuations\n",
    "    no_punc_doc = []\n",
    "    for s in lower_case_doc:\n",
    "        no_punc_doc.append(???)\n",
    "    \n",
    "    # Step 3: tokenize a sentence, i.e., split a sentence into individual words \n",
    "    # using a delimiter. The delimiter specifies what character we will use to identify the beginning \n",
    "    # and the end of a word.\n",
    "    words_doc = []\n",
    "    for s in no_punc_doc:\n",
    "        words_doc.append(???)\n",
    "    \n",
    "    # Step 4: count frequencies. To count the occurrence of each word in the document set. \n",
    "    # We can use the `Counter` method from the Python `collections` library for this purpose. \n",
    "    # `Counter` counts the occurrence of each item in the list and returns a dictionary with \n",
    "    # the key as the item being counted and the corresponding value being the count of that item in the list. \n",
    "    all_words = []\n",
    "    for s in words_doc:\n",
    "        all_words.extend(???)\n",
    "    frequency = \"some function/constructor on all_words\"\n",
    "    \"\"\"\n",
    "\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    lower_case_doc = []\n",
    "    for doc in documents:\n",
    "        lower_case_doc.append(doc.lower())\n",
    "    \n",
    "    no_punc_doc = []\n",
    "    for doc in lower_case_doc:\n",
    "        no_punc_doc.append(doc.translate(str.maketrans('', '', string.punctuation)))\n",
    "        \n",
    "    words_doc = []\n",
    "    for doc in no_punc_doc:\n",
    "        words_doc += doc.split(' ')\n",
    "    \n",
    "    all_words = Counter(words_doc).items()\n",
    "    frequency = dict(all_words)\n",
    "    \n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return frequency\n",
    "\n",
    "# Unit test case:\n",
    "# documents = ['Hello, how are you!', \n",
    "#              'Win money, win from home.',\n",
    "#              'Call me now.',\n",
    "#              'Hello, Call hello you tomorrow?']\n",
    "# sample outputs:\n",
    "# Counter({'hello': 3, 'you': 2, 'win': 2, 'call': 2, 'how': 1, 'are': 1, 'money': 1, 'from': 1, 'home': 1,\n",
    "# 'me': 1, 'now': 1, 'tomorrow': 1})\n",
    "documents = ['Hello, how are you!',\n",
    "            'Win money, win from home.',\n",
    "            'Call me now.',\n",
    "            'Hello, Call hello you tomorrow?']\n",
    "\n",
    "freq = count_frequency(documents)\n",
    "print(freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJ5l72qQ6Prn"
   },
   "source": [
    "#### Step 3: Create training and test sets **(5 points)** {-}\n",
    "\n",
    "We will partition the `SMSSpamCollection` dataset into training and test sets so that we can analyze the model's performance on data it has not witnessed during training. In Lab 1, we have implemented the `split_nfold()` method from scratch for data partition. In this Lab, we will learn to use the `scikit` library. `scikit` is a powerfull tool for machine learning and data mining, providing plenty of well-designed methods for data analysis. We'll use its `train_test_split()` method to create training and testing sets. In this experiment, we use 80% data for training and the remaining 20% data for testing. To ensure your results are replicable, you need to set the `random_state` argument of `train_test_split()` to **1**.\n",
    "\n",
    "There is no cross validation in this Lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "KXkkpzP46Pro",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original dataset contains 5572 examples in total.\n",
      "The training set contains 4457 examples.\n",
      "The testing set contains 1115 examples.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# learn to read API documentation\n",
    "# you can get detailed instructions about this method through this link:\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['sms_message'], df['label'], test_size = 0.2, random_state=1)\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "print(f'The original dataset contains {df.shape[0]} examples in total.')\n",
    "print(f'The training set contains {X_train.shape[0]} examples.')\n",
    "print(f'The testing set contains {X_test.shape[0]} examples.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVB4Giy56Prv"
   },
   "source": [
    "### Implementing Naive Bayes method from scratch {-}\n",
    "\n",
    "#### Step 1: training the Naive Bayes Model **(16 points)** {-}\n",
    "\n",
    "Now that we know what Naive Bayes is, we can take a closer look at how to calculate the posterior probability\n",
    "$$\n",
    "P(Y|X) \\propto P(Y)\\prod_{i=1}^T P(X_i|Y).\n",
    "$$\n",
    "\n",
    "The goal of training is to learn the prior and conditional probability from data. The calculation of the prior $P(Y=y)$ is straightforward. It can be estimated via the frequency of messages in the training set that belong to class $y$, e.g.,\n",
    "$$\n",
    "P(Y=spam) = \\frac{\\# \\text{training messages in the spam category}}{\\# \\text{training messages}}.\n",
    "$$\n",
    "\n",
    "The conditional probability given the class label --- $P(X_i|Y)$ --- can also be estimated from the data by using Eq (5). As we assumed above, it is indeed independent of $i$ (i.e., shared by all $i$). We will leave it to you to translate Eq (5) into a concrete computation scheme. No pseudo-code is provided because by now you should be able to do it. However, do make sure that your implementation complies with the input and output data types as specified in the code.\n",
    "\n",
    "**Hint**: \n",
    "- `count_frequency()` can be useful for computing the conditional probability.\n",
    "- You need to apply the **pseudo-count** trick to handle with unseen words when computing the conditional probability for testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "CATiEZSw6Prw",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.5, 1: 0.5}\n",
      "{0: {'hello': 0.0001999100404817832, 'how': 9.99550202408916e-05, 'are': 9.99550202408916e-05, 'you': 0.0001499325303613374, 'call': 9.99550202408916e-05, 'tomorrow': 9.99550202408916e-05}, 1: {'win': 0.00014994002399040384, 'money': 9.996001599360256e-05, 'from': 9.996001599360256e-05, 'home': 9.996001599360256e-05, 'call': 9.996001599360256e-05, 'me': 9.996001599360256e-05, 'now': 9.996001599360256e-05}}\n"
     ]
    }
   ],
   "source": [
    "def train_NB_model(X_train, y_train):\n",
    "    \"\"\"\n",
    "    training a naive bayes model from the training data.\n",
    "    Inputs:\n",
    "    - X_train: an array of shape (num_train,) which stores SMS messages. each entity is a string type SMS message\n",
    "    - y_train: an array of shape (num_train,). the ground true label for each training data.\n",
    "    Output:\n",
    "    - prior: a dictionary, whose key is the class label, and value is the prior probability.\n",
    "    - conditional: a dictionary whose key is the class label y, and value is another dictionary.\n",
    "                   In the latter dictionary, the key is word w, and the value is the\n",
    "                   conditional probability P(X_i = w | y).\n",
    "    \"\"\"\n",
    "\n",
    "    # To make your code more readable, you can implement some auxiliary functions\n",
    "    # such as `prior_prob` and `conditional_prob` outside of this train_NB_model function\n",
    "\n",
    "    # compute the prior probability\n",
    "    prior = prior_prob(y_train)\n",
    "    \n",
    "    # compute the conditional probability\n",
    "    conditional = conditional_prob(X_train, y_train)\n",
    "\n",
    "    return prior, conditional\n",
    "\n",
    "# Start your auxiliary functions\n",
    "    \n",
    "def prior_prob(y_train):\n",
    "    \"\"\"\n",
    "    compute the prior probability\n",
    "    Inputs:\n",
    "    - y_train: an array that stores ground true label for training data\n",
    "    Outputs:\n",
    "    - prior: a dictionary. key is the class label, value is the prior probability.\n",
    "    \"\"\"\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    prior = {}\n",
    "    \n",
    "    frequency = Counter(y_train)\n",
    "    n = len(y_train)\n",
    "    \n",
    "    for label in frequency.keys():\n",
    "        prior[label] = frequency[label] / n\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****   \n",
    "    \n",
    "    return prior\n",
    "\n",
    "def conditional_prob(X_train, y_train):\n",
    "    \"\"\"\n",
    "    compute the conditional probability for a document set\n",
    "    Inputs:\n",
    "    - X_train: an array of shape (num_train,) which stores SMS messages. each entity is a string type SMS message\n",
    "    - y_train: an array of shape (num_train,). the ground true label for each training data.\n",
    "    Ouputs:\n",
    "    - cond_prob: a dictionary. key is the class label, value is a dictionary in which the key is word, the value is \n",
    "    the conditional probability of feature x_i given y.\n",
    "    \"\"\"\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    alpha, N_alpha = 1, 20000\n",
    "    \n",
    "    spam_messages, ham_messages = [], []\n",
    "\n",
    "    for message, label in zip(X_train, y_train):        \n",
    "        if(label == 1):\n",
    "            spam_messages.append(message)\n",
    "        else:\n",
    "             ham_messages.append(message)\n",
    "    \n",
    "    spam_frequency = count_frequency(spam_messages)\n",
    "    ham_frequency = count_frequency(ham_messages)\n",
    "    \n",
    "    spam_words_count = sum(spam_frequency.values())\n",
    "    ham_words_count = sum(ham_frequency.values())\n",
    "\n",
    "    spam_cond_prob, ham_cond_prob = {}, {}\n",
    "    for word in spam_frequency.keys():\n",
    "        spam_cond_prob[word] = (spam_frequency[word] + alpha) / (spam_words_count + N_alpha)\n",
    "    for word in ham_frequency.keys():\n",
    "        ham_cond_prob[word] = (ham_frequency[word] + alpha) / (ham_words_count + N_alpha)\n",
    "        \n",
    "    cond_prob = {}\n",
    "    cond_prob[0] = ham_cond_prob\n",
    "    cond_prob[1] = spam_cond_prob\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****   \n",
    "    \n",
    "    return cond_prob\n",
    "\n",
    "# unit test case:\n",
    "# x_train = ['Hello, how are you!',\n",
    "#           'Win money, win from home.',\n",
    "#            'Call me now.',\n",
    "#            'Hello, Call hello you tomorrow?']\n",
    "# y_train = np.array([0,1,1,0])\n",
    "#\n",
    "# sample outputs:\n",
    "# prior: {0: 0.5, 1: 0.5}\n",
    "# conditional: {0: {'hello': 0.0001999100404817832, 'how': 9.99550202408916e-05, \n",
    "#                   'are': 9.99550202408916e-05, 'you': 0.0001499325303613374, \n",
    "#                   'call': 9.99550202408916e-05, 'tomorrow': 9.99550202408916e-05}, \n",
    "#               1: {'win': 0.00014994002399040384, 'money': 9.996001599360256e-05, \n",
    "#                   'from': 9.996001599360256e-05, 'home': 9.996001599360256e-05, \n",
    "#                   'call': 9.996001599360256e-05, 'me': 9.996001599360256e-05, \n",
    "#                   'now': 9.996001599360256e-05}}\n",
    "x_train = np.array(['Hello, how are you!',\n",
    "            'Win money, win from home.',\n",
    "            'Call me now.',\n",
    "            'Hello, Call hello you tomorrow?'])\n",
    "y_train_mini = np.array([0,1,1,0])\n",
    "\n",
    "prior = prior_prob(y_train_mini)\n",
    "cond_prob = conditional_prob(x_train, y_train_mini)\n",
    "print(prior)\n",
    "print(cond_prob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdPoLF0-6Pr2"
   },
   "source": [
    "#### Step 2: predict label for test data **(16 points)** {-}\n",
    "\n",
    "Once we have the two models $P(Y)$ and $P(X_i|Y)$ from *training*, we can use them to predict the label for a given test message. To this end, we need to compute the probability of all possible labels, and then predict the one with maximum probability value:\n",
    "$$\n",
    "\\arg\\max_Y P(Y)\\prod_{i=1}^T P(X_i|Y). \\tag{6}\n",
    "$$\n",
    "\n",
    "**Avoid numerical underflow with log-trick.**\n",
    "As shown in the above equation, the calculation involves multiplying many probabilities together. Since probabilities lie in $(0,1]$, multiplying many of them together can lead to numerical underflow (i.e., a floating point number close to 0 gets rounded down to 0 by a computer), especially when $T$ is large, i.e., the test message is long.\n",
    "\n",
    "To overcome this problem, it is common to change the calculation from the product of probabilities to the sum of log probabilities. \n",
    "That is, take the natual logarithm of the right-hand side of Eq (6) as\n",
    "$$\n",
    "g_Y(X) = \\log P(Y) + \\sum_{i=1}^T \\log P(X_i|Y). \\tag{7}\n",
    "$$\n",
    "It is much more numerically stable to compute $g_Y(X)$ and to take $\\arg\\max_Y g_Y(X)$ to find the most likely class label (as the output prediction). \n",
    "\n",
    "With $g_Y(X)$, we can easily compute the posterior probability by\n",
    "$$\n",
    "P(Y|X) = \\frac{\\exp (g_Y(X) - m)}{\\sum_y \\exp (g_y(X)-m)},\n",
    "\\text{ where  }\n",
    "m = \\max_y g_y(X).\n",
    "$$\n",
    "In our message classification problem, the summation in the denominator is just over `positive` and `negative`.\n",
    "Note we subtract by $m$, which does not change the result because the numerator and denominator cancel.\n",
    "However it is numerically useful because sometimes all $g_Y(X)$ are overly small and can cause numerical underflow inside exponentiation.\n",
    "By subtracting $m$, $g_Y(X) - m$ will be 0 (properly scaled) for at least one value of $Y$, and be negative for the other.  And even if another $y$ still suffers underflow in exponentiating $g_y(X)-m$, the posterior probabilities will still be correct.\n",
    "\n",
    "Again, you are expected to implement Eq (7) and loop over all test examples by yourself with no pseudo-code given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "5iDZ9QpB6Pr4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] [[0.9229810319086033, 0.07701896809139684]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import softmax\n",
    "import math\n",
    "def predict_label(X_test, prior_prob, cond_prob):\n",
    "    \"\"\"\n",
    "    predict the class labels for the testing set\n",
    "    Inputs:\n",
    "    - X_test: an array of shape (num_test,) which stores test data. \n",
    "              Each entity is a string type SMS message.\n",
    "    - prior_prob: a dictionary which stores the prior probability for all categories\n",
    "    - cond_prob: a dictionary whose key is the class label y, and value is another dictionary.\n",
    "                   In the latter dictionary, the key is word w, and the value is the\n",
    "                   conditional probability P(X_i = w | y).\n",
    "    Outputs:\n",
    "    - predict: an array that stores predicted labels\n",
    "    - test_prob: an array of shape (num_test, num_classes) which stores the posterior probability of each class\n",
    "    \"\"\"\n",
    "\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    predict, test_prob = [], []\n",
    "    pred_labels = prior_prob.keys()\n",
    "\n",
    "    for doc in X_test:\n",
    "        word_count = count_frequency([doc])\n",
    "        posterior = [0 for _ in pred_labels]\n",
    "        \n",
    "        for i, label in enumerate(pred_labels):            \n",
    "            posterior[i] += compute_test_prob(word_count, prior_prob[label], cond_prob[label])\n",
    "            \n",
    "        max_val = max(posterior)\n",
    "        predict.append(posterior.index(max_val))\n",
    "        test_prob.append(list(softmax([val-max_val for val in posterior])))\n",
    "\n",
    "##     Alternate Approach:\n",
    "#\n",
    "#     for doc in X_test:\n",
    "#         frequency = count_frequency([doc])\n",
    "#         posterior = [math.log(prior_prob[label]) for label in pred_labels]\n",
    "#         for word in frequency:\n",
    "#             for i, label in enumerate(pred_labels):\n",
    "#                 posterior[i] += math.log(cond_prob[label].get(word, (alpha/N_aplha)))\n",
    "        \n",
    "#         max_val = max(posterior)\n",
    "#         predict.append(posterior.index(max_val))\n",
    "#         test_prob.append(list(softmax([val-max_val for val in posterior])))\n",
    "    \n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return predict, test_prob\n",
    "\n",
    "def compute_test_prob(word_count, prior_cat, cond_cat):\n",
    "    \"\"\"\n",
    "    predict the class label for one test example\n",
    "    Inputs:\n",
    "    - word_count: a dictionary which stores the frequencies of each word in a SMS message. \n",
    "                  Key is the word, value is the number of its occurrence in that message\n",
    "    - prior_cat: a scalar. prior probability of a specific category\n",
    "    - cond_cat: a dictionary. conditional probability of a specific category\n",
    "    Outputs:\n",
    "    - prob: posterior probability of a specific category for the test example\n",
    "    \"\"\"\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    alpha, N_alpha = 1, 20000\n",
    "    prob = math.log(prior_cat)\n",
    "    \n",
    "    for key, value in word_count.items():\n",
    "        prob += value * math.log(cond_cat.get(key, (alpha/N_alpha)))\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return prob\n",
    "\n",
    "# unit test case:\n",
    "# x_test = np.array(['Hello, how are you!'])\n",
    "# sample outputs:\n",
    "# y_pred: [0] \n",
    "# prob: [[0.92298104 0.07701896]]\n",
    "x_test = np.array(['Hi, how are you today!'])\n",
    "y_pred, test_prob = predict_label(x_test, prior, cond_prob)\n",
    "print(y_pred, test_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QINYf3KB6Pr-"
   },
   "source": [
    "#### Step 3: compute performance metrics **(5 points)** {-}\n",
    "You may have noticed that the classes are heavily imbalanced. There are only 747 `spam` messages, compared with 4827 `ham` messages. If a classifier simply predicts all messages as `ham`, it will get around 86% accuracy (pretty high). Therefore, accuracy is not a good metric in this case for evaluating the performance of the classifier. As we did before, we can use F-score metrics. But this time we will not implement it from the scratch. Instead, we will learn how to use the builtin methods from `scikit`. [Here](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics) is a summary of well-implemented and commonly used metrics for evaluating the quality of a model's predictions. \n",
    "\n",
    "In this task, you need to **report** the testing accuracy, confusion matrix, and F1 score of the Naive Bayes method by choosing proper functions from `scikit` to compute those metrics with required arguments.\n",
    "\n",
    "Hint: you need to import methods from `sklearn.metrics` before using them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "Wb58oHwb6PsA",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 [[1 2]\n",
      " [1 2]] 0.5714285714285715\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def compute_metrics(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    compute the performance metrics\n",
    "    Inputs:\n",
    "    - y_pred: an array of predictions\n",
    "    - y_true: an array of ground true labels\n",
    "    Outputs:\n",
    "    - acc: accuracy\n",
    "    - cm: confusion matrix\n",
    "    - f1: f1_score\n",
    "    \"\"\"\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return acc, cm, f1\n",
    "\n",
    "# unit test case:\n",
    "# y_pred = np.array([0,1,1,1,0,1])\n",
    "# y_true = np.array([0,1,0,0,1,1])\n",
    "# \n",
    "# sample outputs:\n",
    "# acc: 0.5 \n",
    "# cm: [[1 2]\n",
    "#      [1 2]] \n",
    "# f1: 0.5714285714285715\n",
    "y_pred = np.array([0,1,1,1,0,1])\n",
    "y_true = np.array([0,1,0,0,1,1])\n",
    "acc, cm, f1 = compute_metrics(y_pred, y_true)\n",
    "print(acc, cm, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZLTG_uros32"
   },
   "source": [
    "#### Step 4: Plot ROC curve and print other results **(10 points)** {-}\n",
    "\n",
    "ROC (Receiver Operating Characteristics) curve is one of the most commonly used metrics for evaluating the performance of machine learning algorithms, especially when the classes are imbalanced.\n",
    "\n",
    "ROC is a probability curve for different classes. ROC tells us how good the model is for distinguishing the given classes, in term of the **predicted probability** (not the final hard label in pos/neg). A typical ROC curve has False Positive Rate (FPR) on the $x$-axis and True Positive Rate (TPR) on the $y$-axis. To obtain the FPR and TPR, you can use the `roc_curve` method from `scikit`. This `roc_curve` function takes two arguments: 1) the ground truth labels of the test examples, and 2) the predicted probability that each example is positive. It returns the FPR and TPR which can be used for plotting.\n",
    "\n",
    "You can even compute the area under the curve (AUC) by calling `roc_auc_score` which takes the same arguments as `roc_curve` required.\n",
    "\n",
    "In this task, **plot** the ROC curve and compute the AUC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "6DiYOIdIo7F7",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9047619047619048 0.9748878923766816 0.8947368421052632 [[968   0]\n",
      " [ 28 119]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "def plot_roc_curve(fpr, tpr):\n",
    "  plt.plot(fpr, tpr, color='blue', label='ROC')\n",
    "  plt.plot([0, 1], [0, 1], color='black', linestyle='--')\n",
    "  plt.xlabel('FPR')\n",
    "  plt.ylabel('TPR')\n",
    "  plt.title('ROC Curve')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "# We now compute the test performance.\n",
    "# X_train, X_test, y_train, y_test are the same as above\n",
    "\n",
    "# training naive Bayes model \n",
    "prior, cond = train_NB_model(X_train, y_train)\n",
    "\n",
    "# evaluate on test set\n",
    "y_pred, prob = predict_label(X_test, prior, cond)\n",
    "\n",
    "# Implement the following:\n",
    "#   1. compute the fpr and tpr for roc curve using the probability of being positive\n",
    "#   2. compute the auc score\n",
    "#   3. plot roc curve by calling the plot_roc_curve() method\n",
    "#   4. print AUC score, test accuracy, F-score, and Confusion matrix\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****    \n",
    "fpr, tpr, thres = roc_curve(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "plot_roc_curve(fpr, tpr)\n",
    "acc, cm, f1 = compute_metrics(y_pred, y_test)\n",
    "print(auc, acc, f1, cm)\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJoS8gz6ZXfV"
   },
   "source": [
    "# Problem 2: Linear regression **(38 points)** {-}\n",
    "\n",
    "In this section, we will explore linear regression model. The dataset we will use for this section is Wine Qualuty, whose description can be found [here](http://archive.ics.uci.edu/ml/datasets/Wine+Quality). This dataset contains **4898** examples, each containing **11** features (the first 11 columns), and the **last** (12-th) column is the value we want to predict. The dataset can be downloaded here [`winequality-white.csv`](https://www.cs.uic.edu/~zhangx/teaching/winequality-white.csv) (our code will download it directly). \n",
    "\n",
    "Different from classification models, a regression model is used to predict real values rather than the category an example belongs to. Linear regression is a linear approach to modeling the relationship between features and real value target. To perform supervised learning, we represent the hypothesis as a linear function of features ($x$) to predict the output ($y$).\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = \\theta_0 + \\theta_1x_1 + ... + \\theta_nx_n    \\tag{8}\n",
    "\\end{equation}\n",
    "\n",
    "Here $\\theta_i$'s are the **parameters** parameterizing the space of linear functions mapping from $\\mathcal{X}$ to $\\mathcal{Y}$. Our goal is to **learn** these parameters so that we can find a linear function in this hypothesis space to estimate the output $y$.\n",
    "\n",
    "To simplify the notation and ease the computation, we **pad** the input $x$ by letting $x_0=1$. That is, for an example with three features $x=[x_1, x_2, x_3]$, the padded feature vector will be $x=[1,x_1, x_2, x_3]$. Then, the linear function can be written as:\n",
    "\n",
    "\\begin{equation}\n",
    "f_\\theta(x) = \\sum_{j=0}^n \\theta_j x_j = \\theta^\\top x    \\tag{9}\n",
    "\\end{equation}\n",
    "where on the right-hand side above we are viewing $\\theta$ and $x$ both as vectors, and here $n = 11$ is the number of features. \n",
    "\n",
    "Given a training set, the way to learn these parameters is to make $f_\\theta(x)$ close to $y$. To measure the closeness, we use Mean-Squared-Error (MSE) here. The loss function can therefore be defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "L(\\theta) = \\frac{1}{2m}\\sum_{i=1}^m(f_\\theta(x^{(i)})-y^{(i)})^2 = \\frac{1}{2m}\\sum_{i=1}^m(\\theta^\\top x^{(i)}-y^{(i)})^2,  \\tag{10}\n",
    "\\end{equation}\n",
    "where the superscript $(i)$ denotes the $i$-th example, \n",
    "and $m$ is the total number of training samples. To learn the parameter $\\theta$, our goal is to **minimize** the above loss function. In this lab, we will explore two different methods to learn the parameter: \n",
    "\n",
    "1. Gradient descent\n",
    "2. Closed-form solution (root of the gradient)\n",
    "\n",
    "## 2.1 Data preprocessing {-}\n",
    "\n",
    "Once we have received the dataset, we first need to preprocess it.  Very often, the features in a dataset are of very different scale, which can slow down the optimization for Eq (10). To accelerate it, we need to normalize each feature by substracting its mean value, and then dividing by its standard deviation (std). Assuming $X_i = [x_i^{(1)}, ... , x_i^{(m)}]$ is the $i$-th feature in the training set (across the $m$ examples), the normalized feature $i$ for the $j$-th training example can be computed by:\n",
    "\\begin{equation}\n",
    "\\hat{x}^j_i = \\frac{x^j_i - m_i}{s_i},\n",
    "\\text{ where } m_i = mean(X_i), \\text{ and } s_i = std(X_i).\n",
    "  \\tag{11}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQ3aIqTn4QSt"
   },
   "source": [
    "**Step 1: normalize the training set (3 points)** {-}\n",
    "\n",
    "In the following code block, implement a function `featureNormalization`. The input is the training set. The output is the normalized training set, along with the mean and std of each features. You will need the mean and std to apply to the test set later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eyuuNjr8KITJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def featureNormalization(X):\n",
    "  \"\"\"\n",
    "  Normalize each feature for the input set\n",
    "  Input:\n",
    "  - X: a 2-D numpy array of shape (num_train, num_features)\n",
    "  Outputs:\n",
    "  - X_normalized: a 2-D numpy array of shape (num_train, num_features)\n",
    "  - X_mean: a 1-D numpy array of length (num_features)\n",
    "  - X_std: a 1-D numpy array of length (num_features)\n",
    "  \"\"\"\n",
    "  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "  \n",
    "  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "  \n",
    "  return X_normalized, X_mean, X_std\n",
    "\n",
    "\"\"\"\n",
    " Unit test case\n",
    " Should print\n",
    "[[ 0.49109521 -0.40242644 -0.92156595 -0.1052653 ]\n",
    " [ 0.90298151  1.37532553  1.3897809   1.27398003]\n",
    " [-1.39407672 -0.97289909 -0.46821496 -1.16871473]]\n",
    "[0.99 3.12 4.47 4.51]\n",
    "[0.63124216 2.26128282 1.34553583 3.70492465]\n",
    "\"\"\"\n",
    "X = np.array([[1.30,2.21,3.23,4.12], [1.56, 6.23, 6.34, 9.23], [0.11, 0.92, 3.84, 0.18]])\n",
    "X_normalized, X_mean, X_std = featureNormalization(X)\n",
    "print(X_normalized)\n",
    "print(X_mean)\n",
    "print(X_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "op0kAryrPYgU"
   },
   "source": [
    "**Step 2: normalize the test set (3 points)** {-}\n",
    "\n",
    "The above normalization function will be used for the training set. At test time, we will need to normalize the test data in the same way. However, we shouldn't compute new mean and std from the test set itself, because it may be inconsistent with the training data.  Instead, we will apply the mean $m_i$ and std $s_i$ computed from the training set.  Given a text example $[x_1, \\ldots, x_m]$, we just transform $x_i$ into $(x_i - m_i)/s_i$,\n",
    "where $m_i$ and $s_i$ are computed from the training data as in the *where* clause of Eq (11).\n",
    "\n",
    "In the following code block, implement a function `applyNormalization`, which normalizes the test set for each feature using the provided mean and std."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NWuTPf6yTSaC"
   },
   "outputs": [],
   "source": [
    "def applyNormalization(X, X_mean, X_std):\n",
    "  \"\"\"\n",
    "  Normalize each feature for the input set X\n",
    "  Input:\n",
    "  - X: a 2-D numpy array of shape (num_test, num_features)\n",
    "  - X_mean: a 1-D numpy array of length (num_features)\n",
    "  - X_std: a 1-D numpy array of length (num_features)\n",
    "\n",
    "  Output:\n",
    "  - X_normalized: a 2-D numpy array of shape (num_test, num_features)  \n",
    "  \"\"\"\n",
    "\n",
    "  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "          \n",
    "  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "  return X_normalized\n",
    "\n",
    "\"\"\"\n",
    "Unit test case\n",
    "Should print\n",
    "[[ 3.00e-01  1.21e+00  6.15e-01  4.02e+01]\n",
    " [ 5.60e-01  5.23e+00  2.17e+00  9.13e+01]\n",
    " [-8.90e-01 -8.00e-02  9.20e-01  8.00e-01]]\n",
    "\"\"\"\n",
    "X =  np.array([[1.30,2.21,3.23,4.12], [1.56, 6.23, 6.34, 9.23], [0.11, 0.92, 3.84, 0.18]])\n",
    "X_mean = np.array([1.0, 1.0, 2.0, 0.1])\n",
    "X_std = np.array([1.0, 1.0, 2.0, 0.1])\n",
    "X_normalized = applyNormalization(X, X_mean, X_std)\n",
    "print(X_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVAlHwT_Ts_l"
   },
   "source": [
    "##2.2 Gradient Descent {-}\n",
    "\n",
    "In this section, you will need to implement the gradient descent algorithm that trains the linear regression model. Some introductions to gradient descent can be found [here](https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qP3V48YIt_yo"
   },
   "source": [
    "**Step 1: implement the loss function (3 points)** {-} \n",
    "\n",
    "As introduced at the begining of this problem, we will use MSE to measure the loss. In the following code block, implement a function `computeMSE`. Follow Equation (10), and the function should compute the MSE for the input set with the given $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_I9hUncTPMU"
   },
   "outputs": [],
   "source": [
    "def computeMSE(X, y, theta):\n",
    "  \"\"\"\n",
    "  Compute MSE for the input set (X,y) with theta\n",
    "  Inputs:\n",
    "  - X: a 2-D numpy array of shape (num_samples, num_features+1)\n",
    "  - y: a 1-D numpy array of length (num_samples)\n",
    "  - theta: a 1-D numpy array of length (num_features+1)\n",
    "  Output:\n",
    "  - error: MSE, a real number\n",
    "  \"\"\"\n",
    "  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "  \n",
    "  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "  \n",
    "  return error[0]\n",
    "\n",
    "# Unit test case:\n",
    "# Should print 73.0\n",
    "X = np.array([[2.0, 1.0, 3.0], [3.0, 6.0, 2.0]])\n",
    "y =  np.array([1.0, 1.0])\n",
    "theta = np.array([[1.0], [2.0],[1.0]])\n",
    "error = computeMSE(X, y, theta)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXvZOOiCVyaS"
   },
   "source": [
    "**Step 2: compute the gradient of the loss function (7 points)** {-} \n",
    "\n",
    "Recall that our goal is to find the parameter $\\theta$ that can minimize the loss $L(\\theta)$. To find the $\\theta$ with gradient descent method, we start from some initial $\\theta$, and then repeatedly perform the update:\n",
    "\\begin{equation}\n",
    "\\theta = \\theta - \\alpha\\nabla_{\\theta}L(\\theta).    \\tag{12}\n",
    "\\end{equation}\n",
    "\n",
    "Here $\\alpha > 0$ is a step size, a.k.a., learning rate.\n",
    "To enable this update rule, we first need to compute the gardient in $\\theta$. \n",
    "\n",
    "1. Please derive the gradient of $\\theta$ from Eq (10), and type the result in the following line:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta}L(\\theta) = \n",
    "$$\n",
    "\n",
    "Note for computational efficiency, your expression is not allowed to have $\\sum_{i=1}^m$, and cannot have any multiplication of two matrices.  Multiplications of a matrix and a vector is allowed. Here is a hint.  Let $a_i$ and $b_i$ be vectors.  Then\n",
    "$$\n",
    "  \\sum_i (\\theta^\\top b_i) a_i = \\sum_i a_i (b_i^\\top \\theta) = \\left(\\sum_i a_i b_i^\\top \\right) \\theta = (A B^\\top) \\theta = A (B^\\top \\theta),\n",
    "$$\n",
    "where $A = [a_1, ..., a_m]$ and $B = [b_1, ..., b_m]$.\n",
    "\n",
    "2. Then, implement a function `computeGradient` to compute the gradient $\\nabla_{\\theta}L(\\theta)$ by following the expression you just derived above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9nTxP7YJaCo6"
   },
   "outputs": [],
   "source": [
    "def computeGradient(X, y, theta):\n",
    "  \"\"\"\n",
    "  Compute the gradient of theta\n",
    "  Inputs:\n",
    "  - X: A 2-D numpy array of shape (num_train, num_features+1)\n",
    "  - y: A 1-D numpy array of length (num_train)\n",
    "  - theta: A 1-D numpy array of length (num_features+1)\n",
    "  Output:\n",
    "  - gradient: A 1-D numpy array of length (num_features+1)\n",
    "  \"\"\"\n",
    "\n",
    "  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "  \n",
    "  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "  return gradient\n",
    "\n",
    "\"\"\"\n",
    "# Unit test case:\n",
    "# Should return\n",
    "[[30.]\n",
    " [51.]\n",
    " [25.]]\n",
    "\"\"\" \n",
    "X = np.array([[2.0, 1.0, 3.0], [3.0, 6.0, 2.0]])\n",
    "y =  np.array([1.0, 1.0])\n",
    "theta = np.array([[1.0], [2.0],[1.0]])\n",
    "gradient = computeGradient(X, y, theta)\n",
    "print(gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BCElx6TayKV"
   },
   "source": [
    "**Step 3: implement the gradient descent algorithm (4 points)** {-} \n",
    "\n",
    "Now we can use the update rule in Equation (12) to find the $\\theta$ that minimizes $L(\\theta)$. We start from some initial $\\theta_0$, then repeatedly take a step in the direction of steepest decrease of $L$. The $\\alpha$ in Equation (12) indicates how large the step we want to take at every update. We repeat the updates for a certain number of iterations, and the last updated $\\theta$ will be the $\\theta$ we find.\n",
    "In the following code block, implement a function `gradientDescent`, which updates $\\theta$ for `num_iters` times and records the loss value (MSE) at every iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lcmwpVi3fOoA"
   },
   "outputs": [],
   "source": [
    "def gradientDescent(X, y, theta, alpha, num_iters):\n",
    "  \"\"\"\n",
    "  Update theta using equation (5) for num_iters times.\n",
    "  Input: \n",
    "  - X: a numpy array of shape (num_train, num_features+1)\n",
    "  - y: a numpy array of shape (num_train, 1)\n",
    "  - theta: a 1-D numpy array of length (num_features+1)\n",
    "  - alpha: learning rate, a scalar\n",
    "  - num_iters: an integer specifying how many steps to run the gradient descent\n",
    "  Outputs:\n",
    "  - theta: the final theta, a 1-D numpy array of length (num_features+1). \n",
    "           You can directly overwrite the theta in the input argument, and return it.\n",
    "  - Loss_record: a 1-D numpy array of length (num_iters), \n",
    "          recording the loss value of Eq (10) at every iteration, \n",
    "  \"\"\"\n",
    "  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "  \n",
    "  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "  return theta, Loss_record\n",
    "\n",
    "\"\"\"\n",
    "# Unit test case:\n",
    "# Should return\n",
    "[[0.3322825]\n",
    " [0.858839 ]\n",
    " [0.446925 ]]\n",
    "[37.5778     19.36559064 10.00046345]\n",
    "\"\"\"\n",
    "X = np.array([[2.0, 1.0, 3.0], [3.0, 6.0, 2.0]])\n",
    "y =  np.array([1.0, 1.0])\n",
    "theta = np.array([[1.0], [2.0],[1.0]])\n",
    "alpha = 0.01\n",
    "num_iters = 3\n",
    "theta, Loss_record = gradientDescent(X, y, theta, alpha, num_iters)\n",
    "print(theta)\n",
    "print(Loss_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvegb8zigFpe"
   },
   "source": [
    "## 2.3 Train the linear regression model with gradient descent {-}\n",
    "\n",
    "Now we are ready to chain all the above functions together to perform the linear regression training on the Wine Quality dataset. \n",
    "\n",
    "**Step 1: load the data (not for grading)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0kiPKQ9XHDfg"
   },
   "outputs": [],
   "source": [
    "# First load the data (this code block is not for grading)\n",
    "\n",
    "import urllib.request\n",
    "import shutil\n",
    "\n",
    "url = 'https://www.cs.uic.edu/~zhangx/teaching/winequality-white.csv'\n",
    "file_name = 'winequality-white.csv'\n",
    "with urllib.request.urlopen(url) as response, open(file_name, 'wb') as out_file:\n",
    "    shutil.copyfileobj(response, out_file)\n",
    "\n",
    "data = np.genfromtxt(file_name, delimiter=\";\", skip_header=1)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-0vYx8MJX7c"
   },
   "source": [
    "**Step 2: training and testing (7 points)**\n",
    "\n",
    "After loading the dataset, split the dataset into training and test sets. Please split the first **4000** samples as training set and the rest as test set. Then perform the following:\n",
    "\n",
    "*   Normalize training set features\n",
    "*   Pad the normalized training features by a constant 1, as the new first feature\n",
    "*   Initialize $\\theta$ as a zero vector\n",
    "*   Update $\\theta$ using gradient descent (`num_iters` and `alpha` are provided)\n",
    "*   **Plot** a figure where $x$-axis is the number of iterations, $y$-axis is the loss value (MSE).\n",
    "*   Apply normalization to test set features, pad the features\n",
    "*   Compute the test error (MSE) and **print** out the test error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fYbVsdEbj2By"
   },
   "outputs": [],
   "source": [
    "\n",
    "num_train = 4000\n",
    "alpha = 0.01\n",
    "num_iters = 500\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EePmmYxrkOo4"
   },
   "source": [
    "## 2.4 Effect of different learning rate (4 points) {-}\n",
    "\n",
    "To investigate the effect of learning rate, repeat the learning process (gradient descent) with different learning rate in $[1.0, 0.1, 0.01, 0.001]$. \n",
    "**Plot** 4 figures corresponding to different learning rates, where the $x$-axis is the number of iterations, \n",
    "and the $y$-axis is the loss value (MSE). **Print** the test error (MSE) respectively in the format \"test MSE for using learning rate __ is __\" (four lines in total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y8ANzIIam0NV"
   },
   "outputs": [],
   "source": [
    "learning_rates = [1.0, 0.1, 0.01, 0.001]\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2Ox89manO-n"
   },
   "source": [
    "## 2.5 Closed-form solution  {-}\n",
    "\n",
    "Gradient descent minimizes $L$ by updating $\\theta$ iteratively. There is another way to find the $\\theta$ explicitly. Indeed, by finding the root of the gradient $\\nabla_\\theta L(\\theta)$ (i.e., the $\\theta$ such that $\\nabla_\\theta L(\\theta) = 0$), we can obtain a closed-form solution of $\\theta$ that minimizes the loss $L$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zw9FrxOB7KE_"
   },
   "source": [
    "**Step 1: find the root of the gradient to obtain the closed-form solution of $\\theta$  (5 points)**\n",
    "\n",
    "Type your result in the following lines:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta}L(\\theta) = 0 \\quad \\Rightarrow \\quad \n",
    "\\theta = \n",
    "$$\n",
    "Then, implement a function `closeForm` to compute the closed-form solution of $\\theta$ using the expression you have derived above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HU3ltzQ1r9U3"
   },
   "outputs": [],
   "source": [
    "def closeForm(X, y):\n",
    "  \"\"\"\n",
    "  Compute close form solution for theta\n",
    "  Inputs:\n",
    "  - X: a numpy array of shape (num_train, num_features+1)\n",
    "  - y: a 1-D numpy array of length (num_train)\n",
    "  Output:\n",
    "  - theta: a 1-D numpy array of length (num_features+1)\n",
    "  \"\"\"\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "  \n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "  return theta\n",
    "\n",
    "\"\"\"\n",
    "# Unit test case:\n",
    "# Should return\n",
    "[[ 0.76470588]\n",
    " [-0.17647059]\n",
    " [-0.11764706]]\n",
    "\"\"\"\n",
    "X = np.array([[2.0, 1.0, 3.0], [3.0, 6.0, 2.0], [-1, 0, 2.0]])\n",
    "y = np.array([1.0, 1.0, -1.0])\n",
    "theta = closeForm(X, y)\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUIWxmR0sY4J"
   },
   "source": [
    "**Step 2: evaluate the test error using closed-form solution (2 points)** {-} \n",
    "\n",
    "Compute a new $\\theta$ using the closed-form solution. Evaluate the new $\\theta$ on test set by **printing** the test error (MSE) in the format: \"test MSE using close form solution is : __\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iHtEAKdXtRoD"
   },
   "outputs": [],
   "source": [
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "print('test MSE using close form solution is : ', test_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_si6Qa3wbuut"
   },
   "source": [
    "# Submission Instruction {-}\n",
    "\n",
    "You're almost done! Take the following steps to finally submit your work.\n",
    "\n",
    "1. After executing all commands and completing this notebook, save your `Lab_2.ipynb` as a PDF file, named as `X_Y_UIN.pdf`, where `X` is your first name, `Y` is your last name, and `UIN` is your UIN. Make sure that your PDF file includes all parts of your solution, including the plots. \n",
    "\n",
    "> * Print out all unit test case results before printing the notebook into a PDF.\n",
    "* If you use Colab, open this notebook in Chrome.  Then File -> Print -> set Destination to \"Save as PDF\".  If the web page freezes when printing, close Chrome and reopen the page. If Chrome doesn't work, try Firefox.\n",
    "* If you are working on your own computer, we recommend using the browser (not jupyter) for saving the PDF. For Chrome on a Mac, this is under *File->Print...->Open PDF in Preview*. When the PDF opens in Preview, you can use *Save...* to save it.\n",
    "* Sometimes, a figure that appears near the end of a page can get cut.  In this case, try to add some new lines in the preceding code block so that the figure is pushed to the beginning of the next page.\n",
    "\n",
    "2. Upload `X_Y_UIN.pdf` to Gradescope under `Lab_2_Written`.\n",
    "\n",
    "3. A template of `Lab_2.py` has been provided.  For all functions in `Lab_2.py`, copy the corresponding code snippets you have written into it.  Do not copy any code of plotting figures. **Do not** change the function names.  **Do not** import libraries in Lab_2.py. \n",
    "\n",
    "4. Zip `Lab_2.py` and `Lab_2.ipynb` (**2 files**) into a zip file named `X_Y_UIN.zip`. Suppose the two files are in the folder `Lab_2`.  Then zip up the two files inside the `Lab_2` folder.  Do NOT zip up the folder `Lab_2`. Submit this zip file to Gradescope under `Lab_2_Code`. \n",
    "\n",
    "5. The autograder on Gradscope will be open all the time. We designed some simple test cases to help you check wehther your functions is execuable. You will see the results of running autogrder once you submit your code. Please follow the error messages to debug. If you see 'test `function_name` completed', it means your function is execuable; otherwise please follow the error messages to debug. Since those simple test cases are designed for debugging, it does not guaranttee your solution will work well on the real dataset. It is your responsibility to make your code logically correct. Since all functions are tested in batch, the autograder might take a few minutes to run after submission.\n",
    "\n",
    "6. Only one member of each team needs to submit.  But please specify **all** your teammates on Gradescope. After each submission, please click 'Add Group Member' (right up corner of your submission page) to claim all teammates.\n",
    "\n",
    "You can submit to Gradescope as many times as you would like. We will only consider your last submission."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
